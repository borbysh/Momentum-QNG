{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/borbysh/Momentum-QNG/blob/main/Quantum_Machine_Learning/tutorial_variational_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variational classifier {#variational_classifier}\n\n::: meta\n:property=\\\"og:description\\\": Using PennyLane to implement quantum\ncircuits that can be trained from labelled data to classify new data\nsamples. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_static/demonstration_assets/classifier_output_59_0.png>\n:::\n\n::: related\ntutorial_data_reuploading_classifier Data-reuploading classifier\ntutorial_multiclass_classification Multiclass margin classifier\nensemble_multi_qpu Ensemble classification with Rigetti and Qiskit\ndevices\n:::\n\nIn this tutorial, we show how to use PennyLane to implement variational\nquantum classifiers - quantum circuits that can be trained from labelled\ndata to classify new data samples. The two examples used are inspired by\ntwo of the first papers that proposed variational circuits as supervised\nmachine learning models: [Farhi and Neven\n(2018)](https://arxiv.org/abs/1802.06002) as well as [Schuld et al.\n(2018)](https://arxiv.org/abs/1804.00633).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More precisely, the first example shows that a variational circuit can\nbe optimized to emulate the parity function\n\n$$\\begin{aligned}\nf: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y =\n\\begin{cases} 1 \\text{  if uneven number of 1's in } x \\\\ 0\n\\text{ else}. \\end{cases}\n\\end{aligned}$$\n\nIt demonstrates how to encode binary inputs into the initial state of\nthe variational circuit, which is simply a computational basis state\n(*basis encoding*).\n\nThe second example shows how to encode real vectors as amplitude vectors\ninto quantum states (*amplitude encoding*) and how to train a\nvariational circuit to recognize the first two classes of flowers in the\nIris dataset.\n\n# 1. Fitting the parity function\n\n## Imports\n\nWe start by importing PennyLane, the PennyLane-provided version of\nNumPy, and an optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.optimize import NesterovMomentumOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantum and classical nodes\n\nWe then create a quantum device that will run our circuits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variational classifiers usually define a \"layer\" or \"block\", which is an\nelementary circuit architecture that gets repeated to build the full\nvariational circuit.\n\nOur circuit layer will use four qubits, or wires, and consists of an\narbitrary rotation on every qubit, as well as a ring of CNOTs that\nentangles each qubit with its neighbour. Borrowing from machine\nlearning, we call the parameters of the layer `weights`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(layer_weights):\n    for wire in range(4):\n        qml.Rot(*layer_weights[wire], wires=wire)\n\n    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n        qml.CNOT(wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need a way to encode data inputs $x$ into the circuit, so that\nthe measured output depends on the inputs. In this first example, the\ninputs are bitstrings, which we encode into the state of the qubits. The\nquantum state $\\psi$ after state preparation is a computational basis\nstate that has 1s where $x$ has 1s, for example\n\n$$x = 0101 \\rightarrow |\\psi \\rangle = |0101 \\rangle .$$\n\nThe `~pennylane.BasisState`{.interpreted-text role=\"class\"} function\nprovided by PennyLane is made to do just this. It expects `x` to be a\nlist of zeros and ones, i.e. `[0,1,0,1]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def state_preparation(x):\n    qml.BasisState(x, wires=[0, 1, 2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the variational quantum circuit as this state preparation\nroutine, followed by a repetition of the layer structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef circuit(weights, x):\n    state_preparation(x)\n\n    for layer_weights in weights:\n        layer(layer_weights)\n\n    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to add a \"classical\" bias parameter, the variational quantum\nclassifier also needs some post-processing. We define the full model as\na sum of the output of the quantum circuit, plus the trainable bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def variational_classifier(weights, bias, x):\n    return circuit(weights, x) + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cost\n\nIn supervised learning, the cost function is usually the sum of a loss\nfunction and a regularizer. We restrict ourselves to the standard square\nloss that measures the distance between target labels and model\npredictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def square_loss(labels, predictions):\n    # We use a call to qml.math.stack to allow subtracting the arrays directly\n    return np.mean((labels - qml.math.stack(predictions)) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To monitor how many inputs the current classifier predicted correctly,\nwe also define the accuracy, or the proportion of predictions that agree\nwith a set of target labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def accuracy(labels, predictions):\n    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n    acc = acc / len(labels)\n    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For learning tasks, the cost depends on the data - here the features and\nlabels considered in the iteration of the optimization routine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(weights, bias, X, Y):\n    predictions = [variational_classifier(weights, bias, x) for x in X]\n    return square_loss(Y, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimization\n\nLet's now load and preprocess some data.\n\n::: note\n::: title\nNote\n:::\n\nThe parity dataset\\'s\n`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/parity_train.txt\"\ndownload=parity.txt target=\"_blank\">train</a>`{.interpreted-text\nrole=\"html\"} and\n`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/parity_test.txt\"\ndownload=parity.txt target=\"_blank\">test</a>`{.interpreted-text\nrole=\"html\"} sets can be downloaded and should be placed in the\nsubfolder `variational_classifier/data`.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"variational_classifier/data/parity_train.txt\", dtype=int)\nX = np.array(data[:, :-1])\nY = np.array(data[:, -1])\nY = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n\nfor x,y in zip(X, Y):\n    print(f\"x = {x}, y = {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the variables randomly (but fix a seed for\nreproducibility). Remember that one of the variables is used as a bias,\nwhile the rest is fed into the gates of the variational circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nnum_qubits = 4\nnum_layers = 2\nweights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\nbias_init = np.array(0.0, requires_grad=True)\n\nprint(\"Weights:\", weights_init)\nprint(\"Bias: \", bias_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we create an optimizer instance and choose a batch size...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = NesterovMomentumOptimizer(0.5)\nbatch_size = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and run the optimizer to train our model. We track the accuracy - the\nshare of correctly classified data samples. For this we compute the\noutputs of the variational classifier and turn them into predictions in\n$\\{-1,1\\}$ by taking the sign of the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = weights_init\nbias = bias_init\nfor it in range(100):\n\n    # Update the weights by one optimizer step, using only a limited batch of data\n    batch_index = np.random.randint(0, len(X), (batch_size,))\n    X_batch = X[batch_index]\n    Y_batch = Y[batch_index]\n    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n\n    # Compute accuracy\n    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n\n    current_cost = cost(weights, bias, X, Y)\n    acc = accuracy(Y, predictions)\n\n    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the variational classifier learned to classify all bit\nstrings from the training set correctly.\n\nBut unlike optimization, in machine learning the goal is to generalize\nfrom limited data to *unseen* examples. Even if the variational quantum\ncircuit was perfectly optimized with respect to the cost, it might not\ngeneralize, a phenomenon known as *overfitting*. The art of (quantum)\nmachine learning is to create models and learning procedures that tend\nto find \\\"good\\\" minima, or those that lead to models which generalize\nwell.\n\nWith this in mind, let\\'s look at a test set of examples we have not\nused during training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"variational_classifier/data/parity_test.txt\", dtype=int)\nX_test = np.array(data[:, :-1])\nY_test = np.array(data[:, -1])\nY_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n    \npredictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n\nfor x,y,p in zip(X_test, Y_test, predictions_test):\n    print(f\"x = {x}, y = {y}, pred={p}\")\n    \nacc_test = accuracy(Y_test, predictions_test)\nprint(\"Accuracy on unseen data:\", acc_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The quantum circuit has also learnt to predict all unseen examples\nperfectly well! This is actually remarkable, since the encoding strategy\ncreates quantum states from the data that have zero overlap \\-- and\nhence the states created from the test set have no overlap with the\nstates created from the training set. There are many functional\nrelations the variational circuit could learn from this kind of\nrepresentation, but the classifier chooses to label bit strings\naccording to our ground truth, the parity function.\n\nLet\\'s look at the second example, in which we use another encoding\nstrategy.\n\n# 2. Iris classification\n\nWe now move on to classifying data points from the Iris dataset, which\nare no longer simple bitstrings but represented as real-valued vectors.\nThe vectors are 2-dimensional, but we will add some \\\"latent\ndimensions\\\" and therefore encode inputs into 2 qubits.\n\n## Quantum and classical nodes\n\nState preparation is not as simple as when we represent a bitstring with\na basis state. Every input x has to be translated into a set of angles\nwhich can get fed into a small routine for state preparation. To\nsimplify things a bit, we will work with data from the positive\nsubspace, so that we can ignore signs (which would require another\ncascade of rotations around the Z-axis).\n\nThe circuit is coded according to the scheme in [M\u00f6tt\u00f6nen, et al.\n(2004)](https://arxiv.org/abs/quant-ph/0407010), or---as presented for\npositive vectors only---in [Schuld and Petruccione\n(2018)](https://link.springer.com/book/10.1007/978-3-319-96424-9). We\nalso decomposed controlled Y-axis rotations into more basic gates,\nfollowing [Nielsen and Chuang\n(2010)](http://www.michaelnielsen.org/qcqi/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_angles(x):\n    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n    beta2 = 2 * np.arcsin(np.linalg.norm(x[2:]) / np.linalg.norm(x))\n\n    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n\n\ndef state_preparation(a):\n    qml.RY(a[0], wires=0)\n\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[1], wires=1)\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[2], wires=1)\n\n    qml.PauliX(wires=0)\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[3], wires=1)\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[4], wires=1)\n    qml.PauliX(wires=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test if this routine actually works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0], requires_grad=False)\nang = get_angles(x)\n\n\n@qml.qnode(dev)\ndef test(angles):\n    state_preparation(angles)\n\n    return qml.state()\n\n\nstate = test(ang)\n\nprint(\"x               : \", np.round(x, 6))\nprint(\"angles          : \", np.round(ang, 6))\nprint(\"amplitude vector: \", np.round(np.real(state), 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method computed the correct angles to prepare the desired state!\n\n> ::: note\n> ::: title\n> Note\n> :::\n>\n> The `default.qubit` simulator provides a shortcut to\n> `state_preparation` with the command `qml.StatePrep(x, wires=[0, 1])`.\n> On state simulators, this just replaces the quantum state with our\n> (normalized) input. On hardware, the operation implements more\n> sophisticated versions of the routine used above.\n> :::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are working with only 2 qubits now, we need to update the\n`layer` function. In addition, we redefine the `cost` function to pass\nthe full batch of data to the state preparation of the circuit\nsimultaneously, a technique similar to NumPy broadcasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(layer_weights):\n    for wire in range(2):\n        qml.Rot(*layer_weights[wire], wires=wire)\n    qml.CNOT(wires=[0, 1])\n\n\ndef cost(weights, bias, X, Y):\n    # Transpose the batch of input data in order to make the indexing\n    # in state_preparation work\n    predictions = variational_classifier(weights, bias, X.T)\n    return square_loss(Y, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n\nWe load the Iris data set. There is a bit of preprocessing to do in\norder to encode the inputs into the amplitudes of a quantum state. We\nwill augment the data points by two so-called \\\"latent dimensions\\\",\nmaking the size of the padded data point match the size of the state\nvector in the quantum device. We then need to normalize the data points,\nand finally, we translate the inputs x to rotation angles using the\n`get_angles` function we defined above.\n\nData preprocessing should always be done with the problem in mind; for\nexample, if we do not add any latent dimensions, normalization erases\nany information on the length of the vectors and classes separated by\nthis feature will not be distinguishable.\n\n::: note\n::: title\nNote\n:::\n\nThe Iris dataset can be downloaded\n`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/iris_classes1and2_scaled.txt\"\ndownload=parity.txt target=\"_blank\">here</a>`{.interpreted-text\nrole=\"html\"} and should be placed in the subfolder\n`variational_classifer/data`.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"variational_classifier/data/iris_classes1and2_scaled.txt\")\nX = data[:, 0:2]\nprint(f\"First X sample (original)  : {X[0]}\")\n\n# pad the vectors to size 2^2=4 with constant values\npadding = np.ones((len(X), 2)) * 0.1\nX_pad = np.c_[X, padding]\nprint(f\"First X sample (padded)    : {X_pad[0]}\")\n\n# normalize each input\nnormalization = np.sqrt(np.sum(X_pad**2, -1))\nX_norm = (X_pad.T / normalization).T\nprint(f\"First X sample (normalized): {X_norm[0]}\")\n\n# the angles for state preparation are the features\nfeatures = np.array([get_angles(x) for x in X_norm], requires_grad=False)\nprint(f\"First features sample      : {features[0]}\")\n\nY = data[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These angles are our new features, which is why we have renamed X to\n\"features\" above. Let's plot the stages of preprocessing and play around\nwith the dimensions (dim1, dim2). Some of them still separate the\nclasses well, while others are less informative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\nplt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\nplt.title(\"Original data\")\nplt.show()\n\nplt.figure()\ndim1 = 0\ndim2 = 1\nplt.scatter(X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\nplt.scatter(X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\nplt.title(f\"Padded and normalised data (dims {dim1} and {dim2})\")\nplt.show()\n\nplt.figure()\ndim1 = 0\ndim2 = 3\nplt.scatter(features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\nplt.scatter(features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\nplt.title(f\"Feature vectors (dims {dim1} and {dim2})\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we want to generalize from the data samples. This means that\nwe want to train our model on one set of data and test its performance\non a second set of data that has not been used in training. To monitor\nthe generalization performance, the data is split into training and\nvalidation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nnum_data = len(Y)\nnum_train = int(0.75 * num_data)\nindex = np.random.permutation(range(num_data))\nfeats_train = features[index[:num_train]]\nY_train = Y[index[:num_train]]\nfeats_val = features[index[num_train:]]\nY_val = Y[index[num_train:]]\n\n# We need these later for plotting\nX_train = X[index[:num_train]]\nX_val = X[index[num_train:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimization\n\nFirst we initialize the variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_qubits = 2\nnum_layers = 6\n\nweights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\nbias_init = np.array(0.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we minimize the cost, using the imported optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = NesterovMomentumOptimizer(0.01)\nbatch_size = 5\n\n# train the variational classifier\nweights = weights_init\nbias = bias_init\nfor it in range(60):\n    # Update the weights by one optimizer step\n    batch_index = np.random.randint(0, num_train, (batch_size,))\n    feats_train_batch = feats_train[batch_index]\n    Y_train_batch = Y_train[batch_index]\n    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n\n    # Compute predictions on train and validation set\n    predictions_train = np.sign(variational_classifier(weights, bias, feats_train.T))\n    predictions_val = np.sign(variational_classifier(weights, bias, feats_val.T))\n\n    # Compute accuracy on train and validation set\n    acc_train = accuracy(Y_train, predictions_train)\n    acc_val = accuracy(Y_val, predictions_val)\n\n    if (it + 1) % 2 == 0:\n        _cost = cost(weights, bias, features, Y)\n        print(\n            f\"Iter: {it + 1:5d} | Cost: {_cost:0.7f} | \"\n            f\"Acc train: {acc_train:0.7f} | Acc validation: {acc_val:0.7f}\"\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the continuous output of the variational classifier for the\nfirst two dimensions of the Iris data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\ncm = plt.cm.RdBu\n\n# make data for decision regions\nxx, yy = np.meshgrid(np.linspace(0.0, 1.5, 30), np.linspace(0.0, 1.5, 30))\nX_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n\n# preprocess grid points like data inputs above\npadding = 0.1 * np.ones((len(X_grid), 2))\nX_grid = np.c_[X_grid, padding]  # pad each input\nnormalization = np.sqrt(np.sum(X_grid**2, -1))\nX_grid = (X_grid.T / normalization).T  # normalize each input\nfeatures_grid = np.array([get_angles(x) for x in X_grid])  # angles are new features\npredictions_grid = variational_classifier(weights, bias, features_grid.T)\nZ = np.reshape(predictions_grid, xx.shape)\n\n# plot decision regions\nlevels = np.arange(-1, 1.1, 0.1)\ncnt = plt.contourf(xx, yy, Z, levels=levels, cmap=cm, alpha=0.8, extend=\"both\")\nplt.contour(xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\nplt.colorbar(cnt, ticks=[-1, 0, 1])\n\n# plot data\nfor color, label in zip([\"b\", \"r\"], [1, -1]):\n    plot_x = X_train[:, 0][Y_train == label]\n    plot_y = X_train[:, 1][Y_train == label]\n    plt.scatter(plot_x, plot_y, c=color, marker=\"o\", ec=\"k\", label=f\"class {label} train\")\n    plot_x = (X_val[:, 0][Y_val == label],)\n    plot_y = (X_val[:, 1][Y_val == label],)\n    plt.scatter(plot_x, plot_y, c=color, marker=\"^\", ec=\"k\", label=f\"class {label} validation\")\n\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We find that the variational classifier learnt a separating line between\nthe datapoints of the two different classes, which allows it to classify\neven the unseen validation data with perfect accuracy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
